# ğŸ•·ï¸ Playwright Web Scraper

A comprehensive, production-ready web scraping solution built with Playwright. This scraper can extract all text content from websites, handle JavaScript-rendered pages, and provide both simple and structured data extraction.

## ğŸš€ Quick Start

```bash
# Install dependencies
npm install

# Install browser binaries
npm run install-browsers

# Run basic scraper
npm start

# Run tests
npm test
```

## ğŸ“ Project Structure

```
webscrapper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.js              # Main WebScraper class
â”‚   â”œâ”€â”€ bulk-scraper.js         # Bulk scraping utilities
â”‚   â”œâ”€â”€ configurable-scraper.js # Preset-based scraper
â”‚   â”œâ”€â”€ examples.js             # Usage examples
â”‚   â””â”€â”€ test.js                 # Test suite
â”œâ”€â”€ config.json                 # Configuration presets
â”œâ”€â”€ sample-urls.txt            # Sample URLs for testing
â”œâ”€â”€ package.json               # Dependencies and scripts
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ USAGE.md                   # Detailed usage guide
â””â”€â”€ output/                    # Generated output files
```

## ğŸ› ï¸ Features

### Core Functionality
- âœ… **Multi-browser support** (Chromium, Firefox, WebKit)
- âœ… **JavaScript rendering** - Handles dynamic content
- âœ… **Text extraction** - Clean, formatted text output
- âœ… **Structured extraction** - Headings, paragraphs, links, lists
- âœ… **Bulk processing** - Scrape multiple URLs efficiently
- âœ… **Configuration presets** - Optimized for different site types
- âœ… **Multiple output formats** - JSON, TXT, CSV
- âœ… **Error handling** - Robust error recovery
- âœ… **Rate limiting** - Respectful scraping with delays

### Advanced Features
- ğŸ”§ **Custom selectors** - Wait for specific elements
- ğŸ§¹ **Content filtering** - Exclude unwanted elements
- ğŸ“Š **Progress tracking** - Real-time scraping progress
- ğŸ¯ **Preset configurations** - News, blogs, docs, e-commerce
- ğŸ“ˆ **Performance monitoring** - Track success rates
- ğŸ›¡ï¸ **User agent rotation** - Avoid detection

## ğŸ¯ Use Cases

### 1. Content Migration
```bash
node src/bulk-scraper.js file old-site-urls.txt --structured --output migration.json
```

### 2. Competitive Research
```javascript
import { ConfigurableScraper } from './src/configurable-scraper.js';
const scraper = new ConfigurableScraper('news');
const result = await scraper.scrapeTextStructured('https://competitor.com');
```

### 3. Data Collection
```bash
node src/bulk-scraper.js urls https://site1.com https://site2.com --format csv
```

### 4. Documentation Scraping
```bash
node src/configurable-scraper.js scrape https://docs.example.com documentation
```

## ğŸ“Š Output Examples

### Basic Text Output
```json
{
  "url": "https://example.com",
  "text": "Example Domain This domain is for use in illustrative examples...",
  "length": 187,
  "timestamp": "2025-09-30T07:42:37.768Z"
}
```

### Structured Output
```json
{
  "url": "https://example.com",
  "title": "Example Domain",
  "headings": {
    "h1": ["Example Domain"],
    "h2": ["About This Domain"]
  },
  "paragraphs": ["This domain is for use in illustrative examples..."],
  "links": [{"text": "More information", "href": "https://example.org"}],
  "lists": []
}
```

## ğŸ”§ Configuration

### Available Presets
- **news** - Optimized for news websites
- **ecommerce** - For online stores and product pages
- **blog** - Blog posts and articles
- **documentation** - Technical documentation

### Custom Configuration
```javascript
const scraper = new WebScraper({
  browser: 'chromium',
  headless: true,
  timeout: 30000,
  excludeSelectors: ['script', 'style', '.ads'],
  waitForSelector: '.main-content'
});
```

## ğŸ“š API Reference

### WebScraper Class
- `scrapeText(url)` - Extract plain text
- `scrapeTextStructured(url)` - Extract structured content
- `scrapeMultiplePages(urls)` - Bulk scraping
- `close()` - Cleanup resources

### BulkScraper Class
- `scrapeUrls(urls, options)` - Batch processing
- `scrapeUrlsFromFile(filePath)` - File-based input
- `saveResults(results, filename, format)` - Export data

### ConfigurableScraper Class
- `new ConfigurableScraper(preset, options)` - Preset-based scraper
- `listPresets()` - Available presets
- `getPresetConfig(name)` - Preset configuration

## ğŸ§ª Testing

```bash
# Run all tests
npm test

# Test specific functionality
node src/examples.js

# Test bulk scraping
node src/bulk-scraper.js urls https://example.com
```

## ğŸš¦ Best Practices

1. **Be respectful** - Add delays between requests
2. **Handle errors** - Always use try-catch blocks
3. **Close resources** - Call `await scraper.close()`
4. **Use presets** - Leverage optimized configurations
5. **Validate data** - Check scraped content quality
6. **Monitor performance** - Track success rates
7. **Respect robots.txt** - Check site policies

## ğŸ” Troubleshooting

### Common Issues
- **Timeout errors**: Increase timeout or check connection
- **Empty results**: Try different presets or selectors
- **Memory issues**: Reduce batch size
- **Browser crashes**: Add more delays between requests

### Debug Mode
```javascript
const scraper = new WebScraper({ headless: false });
// Browser window opens for visual debugging
```

## ğŸ“ˆ Performance Tips

- Use headless mode for faster scraping
- Process URLs in batches (3-5 at a time)
- Add 1-2 second delays between batches
- Exclude unnecessary elements early
- Use appropriate browser for the task

## ğŸ¤ Contributing

1. Follow the existing code style
2. Add tests for new features
3. Update documentation
4. Test with multiple browsers
5. Consider performance impact

## ğŸ“„ License

MIT License - Feel free to use in your projects!

---

**Created with â¤ï¸ using Playwright** - The reliable, fast, and modern web scraping solution.